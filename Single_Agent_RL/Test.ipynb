{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import datetime\n",
    "# import platform\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# from mlagents_envs.environment import UnityEnvironment, ActionTuple\n",
    "# from mlagents_envs.side_channel.engine_configuration_channel\\\n",
    "#                              import EngineConfigurationChannel\n",
    "\n",
    "\n",
    "# #파라미터 값 세팅 \n",
    "# state_size = 6*2 # goal plus의 x,z * 3 , goal ex의 x,z *3\n",
    "# action_size = 4 \n",
    "\n",
    "# load_model = True\n",
    "# train_mode = False\n",
    "\n",
    "# discount_factor = 0.9\n",
    "# learning_rate = 0.00025\n",
    "\n",
    "# run_step = 50000 if train_mode else 0\n",
    "# test_step = 5000\n",
    "\n",
    "# print_interval = 10\n",
    "# save_interval = 100\n",
    "\n",
    "# VISUAL_OBS = 0\n",
    "# GOAL_OBS = 1\n",
    "# VECTOR_OBS = 2\n",
    "# OBS = VECTOR_OBS ## A2C에서는 수치적 관측 인덱스를 사용\n",
    "\n",
    "# # 유니티 환경 경로 설정\n",
    "# game = \"GridWorld\"\n",
    "# env_name = \"GridWorld\"\n",
    "\n",
    "# # 모델 저장 및 불러오기 경로\n",
    "# date_time = datetime.datetime.now().strftime(\"%Y_%m%d_%H%M_%S\")\n",
    "# save_path = f\"./saved_models/A2C/{date_time}\"\n",
    "# load_path = f\"./saved_models/A2C/2022_0316_1022_03\"\n",
    "\n",
    "# # 연산 장치\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # A2C 클래스 -> Actor Network, Critic Network 정의 \n",
    "# class A2C(torch.nn.Module):\n",
    "#     def __init__(self, **kwargs):\n",
    "#         super(A2C, self).__init__(**kwargs)\n",
    "#         self.d1 = torch.nn.Linear(state_size, 128)\n",
    "#         self.d2 = torch.nn.Linear(128, 128)\n",
    "#         self.pi = torch.nn.Linear(128, action_size)\n",
    "#         self.v = torch.nn.Linear(128, 1)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.d1(x))\n",
    "#         x = F.relu(self.d2(x))\n",
    "#         return F.softmax(self.pi(x), dim=1), self.v(x)\n",
    "\n",
    "# # A2CAgent 클래스 -> A2C 알고리즘을 위한 다양한 함수 정의 \n",
    "# class A2CAgent:\n",
    "#     def __init__(self):\n",
    "#         self.a2c = A2C().to(device)\n",
    "#         self.optimizer = torch.optim.Adam(self.a2c.parameters(), lr=learning_rate)\n",
    "#         self.writer = SummaryWriter(save_path)\n",
    "\n",
    "#         if load_model == True:\n",
    "#             print(f\"... Load Model from {load_path}/ckpt ...\")\n",
    "#             checkpoint = torch.load(load_path+'/ckpt', map_location=device)\n",
    "#             self.a2c.load_state_dict(checkpoint[\"network\"])\n",
    "#             self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "#     # 정책을 통해 행동 결정 \n",
    "#     def get_action(self, state, training=True):\n",
    "#         #  네트워크 모드 설정\n",
    "#         self.a2c.train(training)\n",
    "\n",
    "#         # 네트워크 연산에 따라 행동 결정\n",
    "#         pi, _ = self.a2c(torch.FloatTensor(state).to(device)) ## state : x_네모, z_네모, x_plus, z_plus, x_ex, z_ex\n",
    "#         action = torch.multinomial(pi, num_samples=1).cpu().numpy() #해당 정책에 따라 랜덤 샘플링 진행 (확률이 높은 index일 수록 샘플링 할때 뽑힐 확률이 크다)\n",
    "#         return action\n",
    "\n",
    "#     # 학습 수행\n",
    "#     def train_model(self, state, action, reward, next_state, done):\n",
    "#         state, action, reward, next_state, done = map(lambda x: torch.FloatTensor(x).to(device),\n",
    "#                                                         [state, action, reward, next_state, done])\n",
    "#         pi, value = self.a2c(state)\n",
    "\n",
    "#         #가치신경망 (actor)\n",
    "#         with torch.no_grad():\n",
    "#             _, next_value = self.a2c(next_state)\n",
    "#             target_value  = reward + (1-done) * discount_factor * next_value\n",
    "#         critic_loss = F.mse_loss(target_value, value)\n",
    "\n",
    "#         #정책신경망 (critic)\n",
    "#         eye = torch.eye(action_size).to(device)\n",
    "#         one_hot_action = eye[action.view(-1).long()]\n",
    "#         advantage = (target_value - value).detach()\n",
    "#         actor_loss = -(torch.log((one_hot_action * pi).sum(1))*advantage).mean()\n",
    "#         total_loss = critic_loss + actor_loss\n",
    "\n",
    "#         self.optimizer.zero_grad()\n",
    "#         total_loss.backward()\n",
    "#         self.optimizer.step()\n",
    "\n",
    "#         return actor_loss.item(), critic_loss.item()\n",
    "\n",
    "#     # 네트워크 모델 저장\n",
    "#     def save_model(self):\n",
    "#         print(f\"... Save Model to {save_path}/ckpt ...\")\n",
    "#         torch.save({\n",
    "#             \"network\" : self.a2c.state_dict(),\n",
    "#             \"optimizer\" : self.optimizer.state_dict(),\n",
    "#         }, save_path+'/ckpt')\n",
    "\n",
    "#         # 학습 기록 \n",
    "#     def write_summray(self, score, actor_loss, critic_loss, step):\n",
    "#         self.writer.add_scalar(\"run/score\", score, step)\n",
    "#         self.writer.add_scalar(\"model/actor_loss\", actor_loss, step)\n",
    "#         self.writer.add_scalar(\"model/critic_loss\", critic_loss, step)\n",
    "\n",
    "# # Main 함수 -> 전체적으로 A2C 알고리즘을 진행 \n",
    "# if __name__ == '__main__':\n",
    "#     # 유니티 환경 경로 설정 (file_name)\n",
    "#     engine_configuration_channel = EngineConfigurationChannel()\n",
    "#     env = UnityEnvironment(file_name=env_name,\n",
    "#                            side_channels=[engine_configuration_channel])\n",
    "#     env.reset()\n",
    "\n",
    "#     # 유니티 브레인 설정 \n",
    "#     behavior_name = list(env.behavior_specs.keys())[0]\n",
    "#     spec = env.behavior_specs[behavior_name]\n",
    "#     engine_configuration_channel.set_configuration_parameters(time_scale=12.0)\n",
    "#     dec, term = env.get_steps(behavior_name)\n",
    "\n",
    "#     # A2C 클래스를 agent로 정의 \n",
    "#     agent = A2CAgent()\n",
    "#     actor_losses, critic_losses, scores, episode, score = [], [], [], 0, 0\n",
    "#     for step in range(run_step + test_step):\n",
    "#         if step == run_step:\n",
    "#             if train_mode:\n",
    "#                 agent.save_model()\n",
    "#             print(\"TEST START\")\n",
    "#             train_mode = False\n",
    "#             engine_configuration_channel.set_configuration_parameters(time_scale=1.0)\n",
    " \n",
    "#         preprocess = lambda obs, goal: np.concatenate((obs*goal[0][0], obs*goal[0][1]), axis=-1) \n",
    "#         state = preprocess(dec.obs[OBS],dec.obs[GOAL_OBS])\n",
    "#         action = agent.get_action(state, train_mode)\n",
    "#         real_action = action + 1\n",
    "#         action_tuple = ActionTuple()\n",
    "#         action_tuple.add_discrete(real_action) \n",
    "#         env.set_actions(behavior_name, action_tuple)\n",
    "#         env.step()\n",
    "\n",
    "#         #환경으로부터 얻는 정보\n",
    "#         dec, term = env.get_steps(behavior_name)\n",
    "#         done = len(term.agent_id) > 0\n",
    "#         reward = term.reward if done else dec.reward\n",
    "#         next_state = preprocess(term.obs[OBS], term.obs[GOAL_OBS]) if done\\\n",
    "#                      else preprocess(dec.obs[OBS], dec.obs[GOAL_OBS])\n",
    "#         score += reward[0]\n",
    "\n",
    "#         if train_mode:\n",
    "#             #학습수행\n",
    "#             actor_loss, critic_loss = agent.train_model(state, action[0], [reward], next_state, [done])\n",
    "#             actor_losses.append(actor_loss)\n",
    "#             critic_losses.append(critic_loss)\n",
    "\n",
    "#         if done:\n",
    "#             episode +=1\n",
    "#             scores.append(score)\n",
    "#             score = 0\n",
    "\n",
    "#           # 게임 진행 상황 출력 및 텐서 보드에 보상과 손실함수 값 기록 \n",
    "#             if episode % print_interval == 0:\n",
    "#                 mean_score = np.mean(scores)\n",
    "#                 mean_actor_loss = np.mean(actor_losses) if len(actor_losses) > 0 else 0\n",
    "#                 mean_critic_loss = np.mean(critic_losses)  if len(critic_losses) > 0 else 0\n",
    "#                 agent.write_summray(mean_score, mean_actor_loss, mean_critic_loss, step)\n",
    "#                 actor_losses, critic_losses, scores = [], [], []\n",
    "\n",
    "#                 print(f\"{episode} Episode / Step: {step} / Score: {mean_score:.2f} / \" +\\\n",
    "#                       f\"Actor loss: {mean_actor_loss:.2f} / Critic loss: {mean_critic_loss:.4f}\")\n",
    "\n",
    "#             # 네트워크 모델 저장 \n",
    "#             if train_mode and episode % save_interval == 0:\n",
    "#                 agent.save_model()\n",
    "#     env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import platform\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from mlagents_envs.environment import UnityEnvironment, ActionTuple\n",
    "from mlagents_envs.side_channel.engine_configuration_channel\\\n",
    "                             import EngineConfigurationChannel\n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # A2C 클래스를 agent로 정의 \n",
    "# agent = A2CAgent()\n",
    "# actor_losses, critic_losses, scores, episode, score = [], [], [], 0, 0\n",
    "# for step in range(run_step + test_step):\n",
    "#     if step == run_step:\n",
    "#         if train_mode:\n",
    "#             agent.save_model()\n",
    "#         print(\"TEST START\")\n",
    "#         train_mode = False\n",
    "#         engine_configuration_channel.set_configuration_parameters(time_scale=1.0)\n",
    "\n",
    "#     preprocess = lambda obs, goal: np.concatenate((obs*goal[0][0], obs*goal[0][1]), axis=-1) \n",
    "#     state = preprocess(dec.obs[OBS],dec.obs[GOAL_OBS])\n",
    "#     action = agent.get_action(state, train_mode)\n",
    "#     real_action = action + 1\n",
    "#     action_tuple = ActionTuple()\n",
    "#     action_tuple.add_discrete(real_action) \n",
    "#     env.set_actions(behavior_name, action_tuple)\n",
    "#     env.step()\n",
    "\n",
    "#     #환경으로부터 얻는 정보\n",
    "#     dec, term = env.get_steps(behavior_name)\n",
    "#     done = len(term.agent_id) > 0\n",
    "#     reward = term.reward if done else dec.reward\n",
    "#     next_state = preprocess(term.obs[OBS], term.obs[GOAL_OBS]) if done\\\n",
    "#                     else preprocess(dec.obs[OBS], dec.obs[GOAL_OBS])\n",
    "#     score += reward[0]\n",
    "\n",
    "#     if train_mode:\n",
    "#         #학습수행\n",
    "#         actor_loss, critic_loss = agent.train_model(state, action[0], [reward], next_state, [done])\n",
    "#         actor_losses.append(actor_loss)\n",
    "#         critic_losses.append(critic_loss)\n",
    "\n",
    "#     if done:\n",
    "#         episode +=1\n",
    "#         scores.append(score)\n",
    "#         score = 0\n",
    "\n",
    "#         # 게임 진행 상황 출력 및 텐서 보드에 보상과 손실함수 값 기록 \n",
    "#         if episode % print_interval == 0:\n",
    "#             mean_score = np.mean(scores)\n",
    "#             mean_actor_loss = np.mean(actor_losses) if len(actor_losses) > 0 else 0\n",
    "#             mean_critic_loss = np.mean(critic_losses)  if len(critic_losses) > 0 else 0\n",
    "#             agent.write_summray(mean_score, mean_actor_loss, mean_critic_loss, step)\n",
    "#             actor_losses, critic_losses, scores = [], [], []\n",
    "\n",
    "#             print(f\"{episode} Episode / Step: {step} / Score: {mean_score:.2f} / \" +\\\n",
    "#                     f\"Actor loss: {mean_actor_loss:.2f} / Critic loss: {mean_critic_loss:.4f}\")\n",
    "\n",
    "#         # 네트워크 모델 저장 \n",
    "#         if train_mode and episode % save_interval == 0:\n",
    "#             agent.save_model()\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 유니티 환경 경로 설정\n",
    "env.close()\n",
    "game = \"Single_Agent_RL\"\n",
    "env_name = \"Single_Agent_RL\"\n",
    "\n",
    "# 유니티 환경 경로 설정 (file_name)\n",
    "engine_configuration_channel = EngineConfigurationChannel()\n",
    "env = UnityEnvironment(file_name=env_name,\n",
    "                        side_channels=[engine_configuration_channel])\n",
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n",
      "[-22.       0.      40.      36.       3.       0.      70.5195]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 유니티 브레인 설정 \n",
    "behavior_name = list(env.behavior_specs.keys())[0]\n",
    "spec = env.behavior_specs[behavior_name]\n",
    "engine_configuration_channel.set_configuration_parameters(time_scale=12.0)\n",
    "dec, term = env.get_steps(behavior_name)\n",
    "\n",
    "\n",
    "print(dec.obs[0][0].shape)\n",
    "print(dec.obs[1][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnityEnvironmentException",
     "evalue": "No Unity environment is loaded.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnityEnvironmentException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-6af290d7a0c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/mlagents_envs/environment.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mUnityEnvironmentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No Unity environment is loaded.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtimed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnityEnvironmentException\u001b[0m: No Unity environment is loaded."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "env.reset()\n",
    "env.step()\n",
    "plt.imshow(dec.obs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "004b330d0a5a8982a6512ffdf0c80aec9b811f72a23a3208ecae245b753181f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
